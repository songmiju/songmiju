- ğŸ‘‹ Hi, Iâ€™m @songmiju
- This is the code for a í…ìŠ¤íŠ¸ìë£Œì²˜ë¦¬ë¶„ì„

## í¬ë¡¤ë§ì½”ë“œ
get_ipython().system('pip install selenium')


from selenium import webdriver
from bs4 import BeautifulSoup
import time
import re
import pandas as pd


url = "https://play.google.com/store/apps/details?id=com.kakao.talk&showAllReviews=true"   # ì ‘ì†í•˜ê³ ìí•˜ëŠ” url
driverPath = "C:\\Users\\stat\\Downloads\\chromedriver_win32\\chromedriver.exe" # Chrome Driver path
driver = webdriver.Chrome(driverPath)   # Open Chrome 
driver.get(url)    #Enter the url



SCROLL_PAUSE_TIME = 3.5 #ëŒ€ê¸°ì‹œê°„ ì§€ì •

last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    #(1) 4ë²ˆì˜  ìŠ¤í¬ë¡¤ë§ì„ í•˜ë„ë¡ í•œë‹¤
    for i in range(4):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(SCROLL_PAUSE_TIME)

    #(2) ë”ë³´ê¸°ë¥¼ í´ë¦­í•˜ë„ë¡ í•´ì¤€ë‹¤ 
    driver.find_element_by_xpath("//span[@class='RveJvd snByac']").click()
    
    #(3) ì¢…ë£Œ ì¡°ê±´
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

reviews = driver.find_elements_by_xpath("//span[@jsname='bN97Pc']")
reviews


dates = driver.find_elements_by_xpath("//span[@class='p2TkOb']")
dates

likes = driver.find_elements_by_xpath("//div[@aria-label='ì´ ë¦¬ë·°ê°€ ìœ ìš©í•˜ë‹¤ëŠ” í‰ê°€ë¥¼ ë°›ì€ íšŸìˆ˜ì…ë‹ˆë‹¤.']")
likes

stars = driver.find_elements_by_xpath("//span[@class='nt2C1d']/div[@class='pf5lIe']/div[@role='img']")
stars  


res_dict = []
#appendë¥¼ ì´ìš©í•˜ì—¬ ìœ„ì—ì„œ ê°€ì ¸ì˜¨ date, star, like, reviewë¥¼ í•©ì³ì¤€ë‹¤.
for i in range(len(reviews)):
    res_dict.append({
        'DATE' : dates[i].text, #í•˜ë‚˜ì”© ë“¤ì–´ê°€ì„œ ê°€ì ¸ì˜¨ë‹¤.
        'STAR' : stars[i].get_attribute('aria-label'),
        'LIKE' : likes[i].text,
        'REVIEW' : reviews[i].text
    })
    
res_df = pd.DataFrame(res_dict)
res_df

res_df.to_csv("ì¹´ì¹´ì˜¤ ì•± ë¦¬ë·° .csv")

get_ipython().system('pip install konlpy')

get_ipython().system('pip install graphviz')


get_ipython().system('pip install wordcloud')


nltk.download('stopwords')

nltk.download('punkt')

import re
import pandas as pd
import numpy as np

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk import ngrams
from konlpy.tag import *

from konlpy.tag import Okt 
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.tree import export_graphviz
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

import os
import matplotlib as plt
import matplotlib.pyplot as plt
from collections import Counter
from wordcloud import WordCloud

import seaborn as sns 
import graphviz
import pickle

import matplotlib.font_manager as font_manager

#ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("C:\\Users\\ì†¡ë¯¸ì£¼\\Desktop\\ì¹´ì¹´ì˜¤ ì•± ë¦¬ë·° .csv", encoding= 'cp949')

#dataframeì—ì„œ reviewë§Œ ë½‘ì•„ì„œ reviewì— ì €ì¥
review = df[['REVIEW']]
review.to_csv("review.txt")

review = open ("review.txt", 'r', encoding= 'utf-8') #ì½ê¸°ì „ìš©('r': read)ìœ¼ë¡œ reviewíŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
reviews = review.read()
print(reviews)


#ë¶ˆìš©ì–´ë¥¼ ì œê±°í•œ í›„ DTM ë§Œë“¤ê¸°
#ì‚¬ìš©ì ì •ì˜ ë¶ˆìš©ì–´ ì‚¬ìš© 

text =[reviews]
stop_words = stopwords.words(['C:\\Users\\ì†¡ë¯¸ì£¼\\Desktop\\stopwords.txt']) #koren stopwordë¥¼ txtíŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ì‚¬ìš©
vect = CountVectorizer(stop_words=['stop_words']) 
m = vect.fit_transform(text)

print(m.toarray()) #returns an ndarray
print(vect.vocabulary_)#ë‹¨ì–´ ì‚¬ì „ ì¶œë ¥


a = sent_tokenize(reviews)#ë‹¨ë½ì„ ë¬¸ì¥ë‹¨ìœ„ë¡œ ë¶„ë¦¬

#ë¬¸ì¥ì— ëŒ€í•œ tokenizerì •ì˜
def tokenizer(raw_texts, pos=["Noun","Verb"], stopword=[]):# í˜•íƒœì†Œê°€ ëª…ì‚¬,ë™ì‚¬ì— í•´ë‹¹ë˜ëŠ” ë‹¨ì–´ ì¶”ì¶œ
    p = okt.pos(raw_texts, 
            norm=True,   # ì •ê·œí™”(normalization)
            stem=True    # ì–´ê°„ì¶”ì¶œ(stemming) ì²˜ë¦¬
            )
    o = [word for word, tag in p if len(word) > 1 and tag in pos and word not in stopword]
    return(o)


#TF-IDFê¸°ë°˜ DTM
vectorize = TfidfVectorizer(
    tokenizer=tokenizer, # ë¬¸ì¥ì— ëŒ€í•œ tokenizer (ìœ„ì—ì„œ ì •ì˜í•œ í•¨ìˆ˜ ì´ìš©)
    min_df=1,            # ë‹¨ì–´ê°€ ì¶œí˜„í•˜ëŠ” ìµœì†Œ ë¬¸ì„œì˜ ê°œìˆ˜
    sublinear_tf=True    # tfê°’ì— 1+log(tf)ë¥¼ ì ìš©í•˜ì—¬ tfê°’ì´ ë¬´í•œì • ì»¤ì§€ëŠ” ê²ƒì„ ë§‰ìŒ
)

X = vectorize.fit_transform(a) # fit_transform í•¨ìˆ˜ë¥¼ í†µí•´Vectorë¡œ ë§Œë“¤ ìˆ˜ ìˆìŒ
X.toarray() #returns an ndarray


#ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì¸¡ì •
cos_x = cosine_similarity(X, X)
cos_x

#í˜•íƒœì†Œë¶„ì„
okt = Okt() #ë‹¨ì–´ë“¤ì„ ì•Œì•„ì„œ ì¡°ê¸ˆ ì •ê·œí™”í•´ì£¼ê³ , ì˜¤íƒ€ë„ ì¡°ê¸ˆ ìˆ˜ì •í•´ì£¼ëŠ” ê¸°ëŠ¥ì´ ìˆìŒ

oo = okt.pos(corpus[0],
        norm=True,   # ì •ê·œí™”(normalization)
        stem=True    # ì–´ê°„ì¶”ì¶œ(stemming)
        )

print(oo)


count = Counter(tok)#ê°¯ìˆ˜ë¥¼ ì„¸ì–´ì¤€ë‹¤
words = dict(count.most_common(50))#ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥í•œë‹¤ , ë°ì´í„°ì˜ ê°œìˆ˜ê°€ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë°°ì—´ì„ ë¦¬í„´ -> 50ê°œ 
words


#ì¹´ì¹´ì˜¤í†¡ ë¦¬ë·°ì— ëŒ€í•œ ì›Œë“œí´ë¼ìš°ë“œ
wordcloud = WordCloud(font_path = 'C:/Windows/Fonts/malgun.ttf',
                      background_color='white',
                      colormap = "Accent_r", 
                      stopwords =stopwords).generate_from_frequencies(words)
plt.imshow(wordcloud) 
plt.show()


#1-grams
n = 1 # n ì˜ ê°œìˆ˜ë¥¼ ì„¤ì •
kko1=[]
sixgrams = ngrams(tok, n)
for grams in sixgrams:
    kko1.append(grams)


kko1=pd.DataFrame(kko1) #ë°ì´í„°í”„ë ˆì„ì„ ë§Œë“¤ì–´ì¤€ë‹¤

kko1['full'] = kko1


count1 = Counter(kko1['full']) #ê°¯ìˆ˜ë¥¼ ì„¸ì–´ì£¼ê³ 
words1 = count1.most_common(20) #ìƒìœ„ 20ê°œë§Œ ë‚˜íƒ€ë‚˜ë„ë¡ í•œë‹¤.
word1 = pd.DataFrame(words1)
word1


plt.figure(figsize=(15,6))
plt.plot(word1[0],word1[1])
plt.grid(True)
plt.title("bigram")
plt.show()

#2-grams
n = 2
kko2=[]
sixgrams = ngrams(tok, n)
for grams in sixgrams:
    kko2.append(grams)

kko2=pd.DataFrame(kko2)



kko2['full'] = kko2[0]+" "+kko2[1]#ì²«ë²ˆì§¸ ë‹¨ì–´ì™€ ë‘ë²ˆì§¸ ë‹¨ì–´ë¥¼ í•©ì³ì„œ fullì´ë¼ëŠ” ë³€ìˆ˜ì— ì €ì¥í•œë‹¤.

count2 = Counter(kko2['full'])
words2 = count2.most_common(20)
word2 = pd.DataFrame(words2)

word2

plt.figure(figsize=(15,6))
plt.plot(word2[0],word2[1])
plt.grid(True)
plt.title("unigram")
plt.show()

#3-grams
n = 3
kko3=[]
sixgrams = ngrams(tok, n)
for grams in sixgrams:
    kko3.append(grams)

kko3=pd.DataFrame(kko3)
kko3['full'] = kko3[0]+" "+kko3[1]+" "+kko3[2]


count3 = Counter(kko3['full'])
words3 = count3.most_common(30)
word3 = pd.DataFrame(words3)


word3

plt.figure(figsize=(15,6))
plt.plot(word3[0],word3[1])
plt.grid(True)
plt.title("trigram")
plt.show()


k = df['STAR'].str.split(' ').str[3] # STARë³€ìˆ˜ë¥¼ ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê³  4ë²ˆì§¸ë§Œ ê°€ì ¸ì˜¨ë‹¤


k = k.str.extract('(\d+)')#ìˆ«ìë§Œ ê°€ì ¸ì˜¨ë‹¤


k= k.astype(float) #floatìœ¼ë¡œ ë°ì´í„° íƒ€ì…ì„ ë°”ê¾¸ì–´ì¤€ë‹¤

df["ë³„ì ìˆ˜"] = k #dfì— ë³„ì ìˆ˜ë¼ëŠ” ë³€ìˆ˜ë¥¼ ë§Œë“¤ì–´ k (ë³„ì ìˆ˜)ë¥¼ ì €ì¥í•œë‹¤.
df

#ë³„ì ìˆ˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def star_preprocessing(value):
    if value <= 3 : #ì—¬ê¸°ì„œ valueëŠ” ë³„ì ìˆ˜ë¥¼ ì˜ë¯¸í•˜ë©° ë³„ì ìˆ˜ê°€ 3ì´í•˜ëŠ” ë¶€ì •ì ì¸ ë¦¬ë·°ë¡œ, 4ì´ìƒì€ ê¸ì •ì ì¸ ë¦¬ë·°ë¡œ ìƒê°í•œë‹¤.
        return '0' 
    else:
        return '1'
    
# í˜•íƒœì†Œ ë¶„ì„ í•¨ìˆ˜ ì •ì˜ 
def tokenizer (text):
    okt =Okt()
    return okt.morphs(text) # morphs : í˜•íƒœì†Œ ì¶”ì¶œ

#train set(í•™ìŠµ ë°ì´í„° ì…‹)ê³¼ test set(í…ŒìŠ¤íŠ¸ ì…‹)ì„ ë¶„ë¦¬
def step1_data_preprocessing() :
    df['rating'] = df['ë³„ì ìˆ˜'].apply(star_preprocessing) #ë³„ì ìˆ˜ ì „ì²˜ë¦¬ë¥¼ í•˜ì—¬ ratingì´ë¼ëŠ” ë³€ìˆ˜ì— ì €ì¥í•œë‹¤.
    text_list =df['REVIEW'].tolist()
    star_list =df['rating'].tolist()
    
    #test_size : í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì˜ ë¹„ìœ¨(float)ì´ë‚˜ ê°¯ìˆ˜(int) (default = 0.25)
    #train_size : í•™ìŠµ ë°ì´í„°ì…‹ì˜ ë¹„ìœ¨(float)ì´ë‚˜ ê°¯ìˆ˜(int) (default = test_sizeì˜ ë‚˜ë¨¸ì§€)
    #random_state : ë°ì´í„° ë¶„í• ì‹œ ì…”í”Œì´ ì´ë£¨ì–´ì§€ëŠ”ë° ì´ë¥¼ ìœ„í•œ ì‹œë“œê°’ (intë‚˜ RandomStateë¡œ ì…ë ¥)
    
    #X_train, X_test, Y_train, Y_test : arraysì— ë°ì´í„°ì™€ ë ˆì´ë¸”ì„ ë‘˜ ë‹¤ ë„£ì—ˆì„ ê²½ìš°ì˜ ë°˜í™˜ì´ë©°, ë°ì´í„°ì™€ ë ˆì´ë¸”ì˜ ìˆœì„œìŒì€ ìœ ì§€ë¨.
    text_train, text_test, star_train, star_test = train_test_split(text_list, star_list, test_size=0.2, random_state=0)
    return text_train, text_test, star_train, star_test


def step2_learning(X_train, y_train, X_test, y_test): 
    
    #ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë‹¨ì–´ ì‚¬ì „ìœ¼ë¡œ ë§Œë“¤ê³  ê° ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ ê³„ì‚°í•œ í›„ ë²¡í„°í™” í•˜ëŠ” ê°ì²´ ìƒì„±
    tfidf = TfidfVectorizer(lowercase=False, tokenizer=tokenizer)
    
    #ë¬¸ì¥ë³„ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ìˆ˜ ì„¸ì„œ ìˆ˜ì¹˜í™”, ë²¡í„°í™”í•´ì„œ í•™ìŠµì„ ì‹œí‚¨ë‹¤.
    logistic = LogisticRegression(C=10.0, penalty='l2', random_state=0) #ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„
    pipe = Pipeline([('vect', tfidf), ('clf', logistic)]) #íŒŒì´í”„ë¼ì¸(pipeline) ê¸°ëŠ¥ì„ ì´ìš©í•˜ì—¬ ë¶„ë¥˜ ëª¨í˜•ê³¼ í•©ì¹  ìˆ˜ ìˆë‹¤
    pipe.fit(X_train, y_train) #X_train, y_trainì´ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ë¥¼ ë§Œë‚˜ fit , transform
    
    # í•™ìŠµ ì •í™•ë„ ì¸¡ì •
    y_pred = pipe.predict(X_test) #test ì‚¬ë¡€ ì˜ˆì¸¡
    print(accuracy_score(y_test, y_pred))
    
    # í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥í•œë‹¤.
    with open('pipe.dat', 'wb') as fp :
        pickle.dump(pipe, fp)
    print('ì €ì¥ì™„ë£Œ')


def step3_using_model() :
    #ê°ì²´ë¥¼ ë³µì›í•œë‹¤.
    with open('pipe.dat', 'rb') as fp:
        pipe = pickle.load(fp)
        while True:
            text = input('ë¦¬ë·°ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš” :')
            str = [text]
            # ì˜ˆì¸¡ ì •í™•ë„
            r1 = np.max(pipe.predict_proba(str) * 100)
            # ì˜ˆì¸¡ ê²°ê³¼
            r2 = pipe.predict(str)[0]
            if r2 == '1' :
                print('ê¸ì •ì ì¸ ë¦¬ë·°')
            else :
                print('ë¶€ì •ì ì¸ ë¦¬ë·°')
                print('ì •í™•ë„ : %.3f' % r1)


#í•™ìŠµ í•¨ìˆ˜
def learning() :
    text_train, text_test, star_train, star_test = step1_data_preprocessing()
    step2_learning(text_train, star_train, text_test, star_test)

#ì‚¬ìš© í•¨ìˆ˜
def using() :
    step3_using_model()

learning()

using()
