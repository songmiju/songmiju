- ğŸ‘‹ Hi, Iâ€™m @songmiju
- This is the code for a CRMê³¼ë°ì´í„°ë§ˆì´ë‹
- R studio

## EDA

stroke = read.csv("C:\\Users\\stat\\Desktop\\stroke.csv")

stroke_re <- stroke %>%
  filter(
    bmi != "N/A",
    gender != "Other"
  ) %>%
  mutate(
    gender = factor(gender),
    hypertension = factor(hypertension),
    heart_disease = factor(heart_disease),
    ever_married = factor(ever_married),
    work_type = factor(work_type),
    Residence_type = factor(Residence_type),
    bmi = as.numeric(bmi),
    smoking_status = factor(smoking_status),
    stroke = factor(stroke),
  )%>%
  select(-id)

## í™•ë¥  êµ¬í•˜ê¸°
dat_prop_gender <- stroke_re %>%
  group_by(gender) %>%
  summarise(prop = sum(stroke == "1")/length(gender))

dat_prop_hypertension <- stroke_re %>%
  group_by(hypertension) %>%
  summarise(prop = sum(stroke == "1")/length(hypertension))

dat_prop_heart_disease <- stroke_re %>%
  group_by(heart_disease) %>%
  summarise(prop = sum(stroke == "1")/length(heart_disease))

dat_prop_ever_married <- stroke_re %>%
  group_by(ever_married) %>%
  summarise(prop = sum(stroke == "1")/length(ever_married))

dat_prop_work_type <- stroke_re %>%
  group_by(work_type) %>%
  summarise(prop = sum(stroke == "1")/length(work_type))

dat_prop_Residence_type <- stroke_re %>%
  group_by(Residence_type) %>%
  summarise(prop = sum(stroke == "1")/length(Residence_type))

dat_prop_smoking_status <- stroke_re %>%
  group_by(smoking_status) %>%
  summarise(prop = sum(stroke == "1")/length(smoking_status))

## ê·¸ë˜í”„
b1 = ggplot(dat_prop_gender, aes(x=gender, y=prop, fill=gender))+geom_col(fill = "#074ca1")

b2 = ggplot(dat_prop_hypertension, aes(x=hypertension, y=prop, fill=hypertension))+geom_col(fill = "#074ca1")

b3 = ggplot(dat_prop_heart_disease, aes(x=heart_disease, y=prop, fill=heart_disease))+geom_col(fill = "#074ca1")

b4 = ggplot(dat_prop_ever_married, aes(x=ever_married,  y=prop, fill=ever_married))+geom_col(fill = "#074ca1")

b5 = ggplot(dat_prop_work_type, aes(x=work_type,y=prop ,fill=work_type))+
   geom_col(fill = "#074ca1")+
   theme(axis.text.x=element_text(angle=45, hjust=1))

b6 = ggplot(dat_prop_Residence_type, aes(x=Residence_type,  y=prop, fill=Residence_type))+geom_col(fill = "#074ca1")

b7 = ggplot(dat_prop_smoking_status, aes(x=smoking_status, y=prop, fill=smoking_status))+
   geom_col(fill = "#074ca1")+
   theme(axis.text.x=element_text(angle=45, hjust=1))

b8 = ggplot(stroke_re, aes(x = stroke, y=bmi))+geom_boxplot(fill='slategrey', color='darkslategrey',width=0.3)

b9 = ggplot(stroke_re, aes(x = stroke, y=avg_glucose_level))+geom_boxplot(fill='slategrey',color='darkslategrey',width=0.3)

grid.arrange(grobs = list(b1, b2, b3,
                          b4, b5, b6,
                          b7, b8, b9),
             ncol = 3,
             top = "EDA"
)

## ë¡œì§€ìŠ¤í‹±íšŒê·€ë¶„ì„
set.seed(214797)

split <- sample(nrow(stroke_re), nrow(stroke_re) * 0.7, replace=F)

fit1 = glm(stroke~.,data=stroke_re , family = "binomial")

fit2 =step(fit1, trace = F)

summary(fit2)

odds = exp(coef(fit2))

knitr::kable(odds, col.names = "ì˜¤ì¦ˆë¹„", caption ="ê° ë³€ìˆ˜ì— ëŒ€í•œ ì˜¤ì¦ˆë¹„")

so = stroke %>%
  filter(
    bmi != "N/A",
    gender != "Other"
  ) %>%
  mutate(
    gender = factor(gender),
    hypertension = factor(hypertension),
    heart_disease = factor(heart_disease),
    ever_married = factor(ever_married),
    work_type = factor(work_type),
    Residence_type = factor(Residence_type),
    bmi = as.numeric(bmi),
    smoking_status = factor(smoking_status),
  )%>%
  select(-id)

tr.idx = sample(nrow(so), 0.5*nrow(so))

x= data.matrix(so[,c(2,3,4,8,9)])

y= so[,11]

fit.lasso <- glmnet(x=x[tr.idx, ], y=y[tr.idx], family="binomial", alpha=1)

fit.ridge <- glmnet(x=x[tr.idx, ], y=y[tr.idx], family="binomial", alpha=0)

fit.elnet <- glmnet(x=x[tr.idx, ], y=y[tr.idx], family="binomial", alpha=.8)


cfit.lasso <- cv.glmnet(x=x[tr.idx, ], y=y[tr.idx], family="binomial", alpha=1)

cfit.ridge <- cv.glmnet(x=x[tr.idx, ], y=y[tr.idx], family="binomial", alpha=0)

cfit.elnet <- cv.glmnet(x=x[tr.idx, ], y=y[tr.idx], family="binomial", alpha=.8)

name = c( "lasso", "ridge", "elnet")
lambda.min = c(cfit.lasso$lambda.min, cfit.ridge$lambda.min, cfit.elnet$lambda.min)

df = data.frame(name, lambda.min)

knitr::kable(df)

fit2.lasso <- glmnet(x=x[tr.idx, ], y=y[tr.idx], family="binomial", alpha=1, lambda=cfit.lasso$lambda.min)

fit2.ridge <- glmnet(x=x[tr.idx, ], y=y[tr.idx], family="binomial", alpha=0, lambda=cfit.ridge$lambda.min)

fit2.elnet <- glmnet(x=x[tr.idx, ], y=y[tr.idx], family="binomial", alpha=.8, lambda=cfit.elnet$lambda.min)

par(mfrow=c(1,2))

plot(fit.elnet, main = "Plot solution paths_elnet", font.main = 4, cex=0.05)

plot(cfit.elnet, main = "ìµœì  ì¡°ìœ¨ëª¨ìˆ˜ (Î»)ì˜ íƒìƒ‰ _ elnet", font.main = 4, cex=0.05)


## ì˜ì‚¬ê²°ì •ë‚˜ë¬´
minority_obs <- stroke_re %>% filter(stroke == 1)

majority_obs <- stroke_re %>% filter(stroke == 0) %>% sample_n(nrow(minority_obs))

balanced_data <- bind_rows(minority_obs, majority_obs)

stroke.rp = rpart(stroke~., balanced_data,
                  subset=split,
                  method = "class")
                  
par(mfrow=c(1,2))

printcp(stroke.rp)

plotcp(stroke.rp)

cp_fit = prune(stroke.rp, cp=0.024)

rpart.plot(cp_fit, main="ì˜ì‚¬ê²°ì •ë‚˜ë¬´", branch.lty=3, shadow.col="gray", 
           nn=TRUE)
           
pred = predict(cp_fit, newdata=balanced_data[-split,], uniform=TRUE)

(t1 <- table(y=balanced_data$stroke[-split], pred=pred[,2]>0.5))

(err <- 1- sum(diag(t1))/sum(t1))


## ë¹„êµ
p.log = prediction(pred.log, stroke_re$stroke[-split])

perf.log = performance(p.log, measure = "tpr", x.measure = "fpr")

auc.log = as.numeric(performance(p.log, "auc")@y.values)

p = prediction (pred[,2], balanced_data$stroke[-split])

perf = performance(p ,measure ="tpr", x.measure = "fpr")

auc = as.numeric(performance(p, "auc")@y.values)

plot(perf, col="red",xlab="FPR (Flase Positive Rate)" , ylab="TPR(True Positive rate)")

plot(perf.log, add = TRUE, col="blue")

abline(0,1)

text(0.5, 0.5, paste("AUC of tree:",round(auc,4)))

text(0.5, 0.4, paste("AUC of logistic:",round(auc.log,4)))

## ê¸°íƒ€ ê°ë…í•™ìŠµ ëª¨í˜•
stroke_cat = stroke%>%
  select(age, 
         hypertension, 
         heart_disease, 
         avg_glucose_level, 
         bmi, 
         stroke)

stroke_nu = stroke %>%
  select(age, hypertension, heart_disease, avg_glucose_level, bmi, stroke) 
  
stroke_cat1 = stroke_cat

stroke_cat1$age = cut(stroke$age, c(-Inf, median(stroke$age), Inf))

stroke_cat1$hypertension =  cut(stroke$hypertension, c(-Inf, median(stroke$hypertension), Inf))

stroke_cat1$heart_disease= cut(stroke$heart_disease, c(-Inf, median(stroke$heart_disease), Inf))

stroke_cat1$avg_glucose_level = cut(stroke$avg_glucose_level, c(-Inf, median(stroke$avg_glucose_level), Inf))

stroke$bmi = ifelse(is.na(stroke$bmi), 28.89324, stroke$bmi)

stroke$bmi=as.numeric(stroke$bmi)

stroke_cat$bmi = cut(stroke$bmi, c(-Inf, median(stroke$bmi), Inf))

model = naiveBayes(stroke ~ ., data = stroke_cat)

predict(model, stroke_cat[1:6,-6], type = "raw") #ì‚¬í›„í™•ë¥  ì¶”ì •

pred = predict(model, stroke_cat[,-6])

cm = table(pred, stroke$stroke)

cm

 1- sum(diag(cm)/sum(cm))

## ê²€ì¦ ìë£Œì— ëŒ€í•´ ì˜ˆì¸¡

#using laplace smoothing:

#model2 =  naiveBayes(stroke ~ ., data = stroke_cat, laplace = 3)

#pred = predict(model, stroke_nu[,-6])

#lap = table(pred, stroke$stroke)

#1- sum(diag(lap)/sum(lap))

tr.idx = sample(nrow(stroke), 0.5*nrow(stroke))

stroke1 = nnet(stroke~., data=stroke_knn[tr.idx,], size = 2, decay = 5e-4)

pred = predict(stroke1, stroke_knn[-tr.idx, -6], type="class")

nn = table(stroke_knn$stroke[-tr.idx], pred)

1- sum(diag(nn))/sum(nn)

#tr.idx = sample(nrow(stroke), 0.5*nrow(stroke))
#obj = tune(svm, stroke ~ ., 
#data = stroke_nu[tr.idx,], 
#type = "prob",          
#ranges = list(cost = c(0.25, 0.35)))
#model <- svm(stroke ~., data = stroke_nu[tr.idx,],
#cost=obj$best.parameters$cost)
#summary(model)
#pred <- predict(model, newdata=so[-tr.idx,])
#table(so$stroke[-tr.idx], pred)

svm_model <- svm(stroke ~. , data = stroke_nu[tr.idx,], type = "C-classification", kernel = "radial")

svm_training_prediction <- predict(svm_model, newdata =  stroke_nu[tr.idx,])
svm_training_error <- mean(svm_training_prediction != stroke_nu[tr.idx,]$stroke)

svm_training_error

## ì•™ìƒë¸”ë°©ë²•(Ensemble)
stroke_nu$stroke = as.integer(stroke_nu$stroke==1)

stroke_nu$bmi = ifelse(is.na(stroke$bmi), 28.89324, stroke$bmi)
stroke_nu$bmi=as.numeric(stroke$bmi)
gbm1 <- gbm(stroke ~ ., data=stroke_nu[tr.idx, ], 
        distribution = "bernoulli",
        n.trees = 500, 
        bag.fraction = 0.5, 
        train.fraction = 0.5,  
        n.minobsinnode = 10, 
        cv.folds = 5, 
        keep.data = TRUE, 
        verbose = FALSE, 
        n.cores = 1)

best.iter = gbm.perf(gbm1, method="cv")

pretty.gbm.tree(gbm1, i.tree = 21)

pred.boo <- predict(gbm1, newdata=stroke_nu[-tr.idx, -6], type="response")
( t_boo <- table(prediction=pred.boo>0.5, actural=stroke_nu$stroke[-tr.idx]))
1-sum(diag(t_boo))/sum(t_boo)
p_boo <- prediction(pred.boo, stroke_nu$stroke[-tr.idx])
roc_boo <- performance(p_boo, measure = "tpr", x.measure = "fpr") 
auc_boo <- round(performance(p_boo, measure = "auc")@y.values[[1]], 4)
plot(roc_boo, main=paste0("ROC of gbm (AUC=", auc_boo, ")"))

ref.label = â€œa3â€
pred.log <- predict(fit2, newdata=stroke_re[-split,], type=â€œresponseâ€)

(t2 <- table(y=stroke_re$stroke[-split], pred=pred.log>0.5))

(err <- 1- sum(diag(t2))/sum(t2))

ref.label = â€œa5â€
pred.elnet <- predict(fit2.elnet, newx=x[-tr.idx, ], type=â€œresponseâ€, s=â€œLambda.minâ€)

plot(pred.elnet)

lines(pred.elnet, col=â€œskyblueâ€)

tt = table(pred.elnet>0.5, y[-tr.idx])

(err <- 1- sum(diag(tt))/sum(tt))

ref.label = â€œknnâ€
set.seed(214797)

tr.idx = sample(nrow(stroke), 0.5*nrow(stroke))

stroke_knn = stroke_re%>%

select(age, hypertension, heart_disease, avg_glucose_level, bmi, stroke)

y = stroke_knn[,6]

tr.idx = sample(length(y), nrow(stroke_knn)/2)
result <- numeric()

k = 5:50

for(i in k ){

wdbc_pred <- knn(wdbc_train, wdbc_test, wdbc_train_y, k=i)

t <- table(wdbc_pred, wdbc_test_y)

result[i-4] <- (t[1,1]+t[2,2])/sum(t)

}

sort(result, decreasing = T)

which(result==max(result))
pred = knn(train=stroke_knn[tr.idx, -6], test=stroke_knn[-tr.idx,-6], cl=y[tr.idx], k = 9)

kn = table(pred, y[-tr.idx])

kn

1- sum(diag(kn))/sum(kn)

ref.label = â€œbagâ€
m.bag = bagging(stroke~., data=stroke[tr.idx, ], nbagg=500, control=list(maxdepth=1))
par(mfrow=c(2, 3))

for(i in 1:5){

rpart.plot(m.bag\(mtrees[[i]]\)btree, branch.lty=3, nn=TRUE)

}
pred_bag <- predict(m.bag, newdata=stroke[-tr.idx, -12], type=â€œprobâ€) #class

( t_bag <- table(prediction=pred_bag>0.5, actural=stroke$stroke[-tr.idx]) )

1-sum(diag(t_bag))/sum(t_bag)
p_bag <- prediction(pred_bag, stroke$stroke[-tr.idx])

roc_bag <- performance(p_bag, measure = â€œtprâ€, x.measure = â€œfprâ€)

auc_bag <- round(performance(p_bag, measure = â€œaucâ€)@y.values [[1]], 4)

plot(roc_bag, main=paste0(â€œROC of gbm (AUC=â€, auc_bag, â€œ)â€))

ref.label = â€œrandâ€
stroke_nu\(stroke = factor(stroke_nu\)stroke)

stroke_nu\(bmi = ifelse(is.na(stroke\)bmi), 28.89324, stroke\(bmi) m.rf <- randomForest(x=stroke_nu[tr.idx, -6], y=stroke_nu\)stroke[tr.idx],

ntree=100,

mtry=5,

importance=TRUE)

varImpPlot(m.rf, type=1)

importance(m.rf)

imp <- importance(m.rf, type=1)

impvar <- rownames(imp)[order(imp[, 1], decreasing=TRUE)]

par(mfrow=c(2, 3))

for (i in 1:5){

partialPlot(m.rf, pred.data=stroke_nu[tr.idx,], impvar[i], xlab=impvar[i],

main=paste(â€œPartial Dependence onâ€, impvar[i]))

}
pred.rf <- predict(m.rf, newdata=stroke_nu[-tr.idx, -6], type=â€œprobâ€)[,2]

(t_rf <- table(prediction=pred.rf>0.5, actural=stroke_nu$stroke[-tr.idx]))
p_rf <- prediction(pred.rf, stroke_nu$stroke[-tr.idx])

roc_rf <- performance(p_rf, measure = â€œtprâ€, x.measure = â€œfprâ€)

auc_rf <- round(performance(p_rf, measure = â€œaucâ€)@y.values [[1]], 4)

plot(roc_rf, main=paste0(â€œROC of randomforest (AUC=â€, auc_rf, â€œ)â€))

ref.label = â€œresultâ€
err <- c(sum(diag(t_bag))/sum(t_bag),

sum(diag(t_boo))/sum(t_boo),

sum(diag(t_rf))/sum(t_rf))

auc <- c(auc_bag, auc_boo, auc_rf)
p <- data.frame(Error Rate=err, AUC=auc)

rownames(p) <- c(â€œbaggingâ€, â€œgbmâ€, â€œRandom Forestâ€)
